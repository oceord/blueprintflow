{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab26265",
   "metadata": {},
   "source": [
    "# BlueprintFlow: Data Bootstrap\n",
    "\n",
    "This notebook serves as an example demonstrating how to systematically bootstrap data into BlueprintFlow for demonstration purposes.\n",
    "\n",
    "## What You'll Bootstrap\n",
    "\n",
    "- **Language Context**: A language context for Python ETL development\n",
    "- **Tool Stack**: Preferred libraries and tools for ETL processes\n",
    "- **Coding Standards**: Rules that must be followed in the codebase and guidelines and coding standards for Python ETL development\n",
    "- **Project Structure**: Source code structure for an ETL project\n",
    "- **Design Patterns**: Common abstractions and design patterns useful in ETL processes\n",
    "- **Implementation Examples**: Example code implementations for extract, transform, and load operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9f213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49d44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb_util import pprint_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339e3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blueprintflow.core.models.tasks import (\n",
    "    CreateAbstractionTask,\n",
    "    CreateCodeTask,\n",
    "    CreateGuidelineTask,\n",
    "    CreateLanguageContextTask,\n",
    "    CreatePreferenceTask,\n",
    "    CreateRuleTask,\n",
    "    CreateSrcStructureTask,\n",
    ")\n",
    "from blueprintflow.core.settings import load_settings\n",
    "from blueprintflow.store.store_manager import StoreManager\n",
    "from blueprintflow.utils.xdg.data import UserData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d5465",
   "metadata": {},
   "source": [
    "## Initialize BlueprintFlow Components\n",
    "\n",
    "This section sets up the necessary components for interacting with BlueprintFlow's database:\n",
    "- Loads default settings\n",
    "- Initializes the default user data storage\n",
    "- Creates a store manager to handle database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d7ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_settings = load_settings()\n",
    "default_user_data = UserData(reset_if_exists=True)\n",
    "store_manager = StoreManager(default_settings, default_user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf686c",
   "metadata": {},
   "source": [
    "## Define Language Context for Python ETL\n",
    "\n",
    "Establish the core language context for the blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a509cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl',\n",
      " 'language': 'python',\n",
      " 'context': 'etl',\n",
      " 'description': 'Python is a versatile and readable language ideal for ETL '\n",
      "                'processes. It excels in handling data extraction from diverse '\n",
      "                'sources, transforming data into required formats, and loading '\n",
      "                'it into target systems. Its flexibility and extensive '\n",
      "                'ecosyste [...]',\n",
      " 'embedding': [-0.00034710637, -0.02914067, 0.055726133]}\n"
     ]
    }
   ],
   "source": [
    "lang_context_key = \"python_etl\"\n",
    "create_lang_context_task = CreateLanguageContextTask(\n",
    "    key=lang_context_key,\n",
    "    language=\"python\",\n",
    "    context=\"etl\",\n",
    "    description=\"Python is a versatile and readable language ideal for ETL processes. \"\n",
    "    \"It excels in handling data extraction from diverse sources, transforming data \"\n",
    "    \"into required formats, and loading it into target systems. \"\n",
    "    \"Its flexibility and extensive ecosystem make it a popular choice for building \"\n",
    "    \"efficient data processing workflows and integration tasks.\",\n",
    ")\n",
    "_, lang_context = store_manager.create(create_lang_context_task)\n",
    "pprint_model(lang_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12a3fb",
   "metadata": {},
   "source": [
    "## Define Preferred Libraries\n",
    "\n",
    "Specifies preferred libraries for ETL operations in Python:\n",
    "\n",
    "- Polars: A high-performance DataFrame library optimized for large datasets\n",
    "- DuckDB: An embedded analytical database with SQL interface and efficient data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f40e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '5ee11786-634a-434a-baa5-b8265985b51d',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'polars',\n",
      " 'description': 'A fast DataFrame library for Python designed as a powerful '\n",
      "                'tool for ETL tasks. It excels in efficient data '\n",
      "                'transformation and loading, powered by Rust for high '\n",
      "                'performance with large datasets. Adopts an imperative '\n",
      "                'programming approach, ideal fo [...]',\n",
      " 'tags': ['dataframe', 'performance', 'big data'],\n",
      " 'embedding': [-0.020702168, 0.028024463, 0.0018910654]}\n"
     ]
    }
   ],
   "source": [
    "create_polars_preference_task = CreatePreferenceTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"polars\",\n",
    "    description=\"A fast DataFrame library for Python designed as a powerful tool for \"\n",
    "    \"ETL tasks. It excels in efficient data transformation and loading, powered by \"\n",
    "    \"Rust for high performance with large datasets. Adopts an imperative programming \"\n",
    "    \"approach, ideal for developers who prefer native data manipulation and control \"\n",
    "    \"within Python.\",\n",
    "    tags=[\"dataframe\", \"performance\", \"big data\", \"Rust\", \"imperative\"],\n",
    ")\n",
    "_, polars_preference = store_manager.create(create_polars_preference_task)\n",
    "pprint_model(polars_preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d731c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '739cd794-0dda-4ad2-8946-975b149e232b',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'duckdb',\n",
      " 'description': 'An embedded, in-memory analytical database with columnar '\n",
      "                'storage, optimized for fast analytical queries and seamless '\n",
      "                'Python integration. Excels in ETL by efficiently processing '\n",
      "                'diverse data formats such as CSV, Parquet, Delta, and JSON. '\n",
      "                'Its lig [...]',\n",
      " 'tags': ['database', 'columnar', 'sql'],\n",
      " 'embedding': [-0.04089327, -0.0075872457, 0.011690059]}\n"
     ]
    }
   ],
   "source": [
    "create_duckdb_preference_task = CreatePreferenceTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"duckdb\",\n",
    "    description=\"An embedded, in-memory analytical database with columnar storage, \"\n",
    "    \"optimized for fast analytical queries and seamless Python integration. Excels in \"\n",
    "    \"ETL by efficiently processing diverse data formats such as CSV, Parquet, Delta, \"\n",
    "    \"and JSON. Its lightweight design and high-speed batch processing make it ideal \"\n",
    "    \"for cost-effective, lightweight ETL workflows. Supports a fully declarative \"\n",
    "    \"SQL-based approach, enabling complex data operations using SQL syntax. Integrates \"\n",
    "    \"smoothly with other tools, making it versatile for comprehensive data \"\n",
    "    \"transformation and analysis.\",\n",
    "    tags=[\n",
    "        \"database\",\n",
    "        \"columnar\",\n",
    "        \"sql\",\n",
    "        \"json\",\n",
    "        \"ETL\",\n",
    "        \"performance\",\n",
    "        \"lightweight\",\n",
    "        \"declarative\",\n",
    "    ],\n",
    ")\n",
    "_, duckdb_preference = store_manager.create(create_duckdb_preference_task)\n",
    "pprint_model(duckdb_preference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e092b4a",
   "metadata": {},
   "source": [
    "## Define Rules and Standards\n",
    "\n",
    "Establish strict coding rules that must be followed in the ETL implementation including:\n",
    "\n",
    "- Documentation standards (Google-style docstrings)\n",
    "- Type safety requirements (static typing for functions)\n",
    "- Code quality rules (no assert statements outside tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b439f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '493b927a-ca89-41b2-84d9-deb13566368f',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Google convention docstrings',\n",
      " 'description': 'Always generate docstrings following the Google convention',\n",
      " 'rule_type': 'documentation',\n",
      " 'violations_action': 'require_fix',\n",
      " 'embedding': [-0.058820747, -0.042512137, -0.018018302]}\n"
     ]
    }
   ],
   "source": [
    "create_docstring_rule_task = CreateRuleTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Google convention docstrings\",\n",
    "    description=\"Always generate docstrings following the Google convention\",\n",
    "    rule_type=\"documentation\",\n",
    "    violations_action=\"require_fix\",\n",
    ")\n",
    "_, docstring_rule = store_manager.create(create_docstring_rule_task)\n",
    "pprint_model(docstring_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c2d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '35f229b6-a0bc-4199-9887-b733668016e0',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Static typing for functions',\n",
      " 'description': 'Always generate static typing for function definitions to be '\n",
      "                'evaluated with mypy later on',\n",
      " 'rule_type': 'typing',\n",
      " 'violations_action': 'require_fix',\n",
      " 'embedding': [-0.026746703, 0.0070767906, 0.05300787]}\n"
     ]
    }
   ],
   "source": [
    "create_typing_rule_task = CreateRuleTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Static typing for functions\",\n",
    "    description=\"Always generate static typing for function definitions to be \"\n",
    "    \"evaluated with mypy later on\",\n",
    "    rule_type=\"typing\",\n",
    "    violations_action=\"require_fix\",\n",
    ")\n",
    "_, typing_rule = store_manager.create(create_typing_rule_task)\n",
    "pprint_model(typing_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e900a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'f18bea79-b8d1-4644-9835-69e27fa00f5e',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'No assert outside tests',\n",
      " 'description': 'Do not use assert outside of test functions',\n",
      " 'rule_type': 'style',\n",
      " 'violations_action': 'require_fix',\n",
      " 'embedding': [-0.010566957, -0.016409732, 0.036240608]}\n"
     ]
    }
   ],
   "source": [
    "create_assert_rule_task = CreateRuleTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"No assert outside tests\",\n",
    "    description=\"Do not use assert outside of test functions\",\n",
    "    rule_type=\"style\",\n",
    "    violations_action=\"require_fix\",\n",
    ")\n",
    "_, assert_rule = store_manager.create(create_assert_rule_task)\n",
    "pprint_model(assert_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7050f",
   "metadata": {},
   "source": [
    "## Establish Guidelines\n",
    "\n",
    "Define coding guidelines and best practices for Python ETL development including:\n",
    "\n",
    "- The Zen of Python principles\n",
    "- Reduced memory footprint\n",
    "- Prefer lazy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c951e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'd9673d67-7201-4a5d-8869-822184a8a71c',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'The Zen of Python',\n",
      " 'description': 'Beautiful is better than ugly.\\n'\n",
      "                'Explicit is better than implicit.\\n'\n",
      "                'Simple is better than complex.\\n'\n",
      "                'Complex is better than complicated.\\n'\n",
      "                'Flat is better than nested.\\n'\n",
      "                'Sparse is better than dense.\\n'\n",
      "                'Readability counts.\\n'\n",
      "                \"Special cases aren't special enough [...]\",\n",
      " 'category': 'principles',\n",
      " 'examples': None,\n",
      " 'embedding': [-0.07254717, -0.003949336, 0.05075067]}\n"
     ]
    }
   ],
   "source": [
    "create_zen_guideline_task = CreateGuidelineTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"The Zen of Python\",\n",
    "    description=inspect.cleandoc(\n",
    "        \"\"\"\n",
    "        Beautiful is better than ugly.\n",
    "        Explicit is better than implicit.\n",
    "        Simple is better than complex.\n",
    "        Complex is better than complicated.\n",
    "        Flat is better than nested.\n",
    "        Sparse is better than dense.\n",
    "        Readability counts.\n",
    "        Special cases aren't special enough to break the rules.\n",
    "        Although practicality beats purity.\n",
    "        Errors should never pass silently.\n",
    "        Unless explicitly silenced.\n",
    "        In the face of ambiguity, refuse the temptation to guess.\n",
    "        There should be one-- and preferably only one --obvious way to do it.\n",
    "        Although that way may not be obvious at first unless you're Dutch.\n",
    "        Now is better than never.\n",
    "        Although never is often better than *right* now.\n",
    "        If the implementation is hard to explain, it's a bad idea.\n",
    "        If the implementation is easy to explain, it may be a good idea.\n",
    "        Namespaces are one honking great idea -- let's do more of those!\"\"\"\n",
    "    ),\n",
    "    category=\"principles\",\n",
    ")\n",
    "_, zen_guideline = store_manager.create(create_zen_guideline_task)\n",
    "pprint_model(zen_guideline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a678b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '8add6074-6fa9-4e0b-ad69-388128e142af',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Reduce Memory Footprint',\n",
      " 'description': 'Reduce the memory footprint in Python ETL processes by '\n",
      "                'implementing best practices such as preferring to iterate '\n",
      "                'over batches instead of loading everything all at once, using '\n",
      "                'generators instead of lists whenever possible, choosing '\n",
      "                'memory-effici [...]',\n",
      " 'category': 'Optimization',\n",
      " 'examples': None,\n",
      " 'embedding': [-0.019494656, -0.009317339, 0.02538765]}\n"
     ]
    }
   ],
   "source": [
    "create_memory_guideline_task = CreateGuidelineTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Reduce Memory Footprint\",\n",
    "    description=\"Reduce the memory footprint in Python ETL processes by implementing \"\n",
    "    \"best practices such as preferring to iterate over batches instead of loading \"\n",
    "    \"everything all at once, using generators instead of lists whenever possible, \"\n",
    "    \"choosing memory-efficient data structures, and avoiding unnecessary data copies. \"\n",
    "    \"Always check if memory optimizations can impact performance, and if they do, add \"\n",
    "    \"the following comments to indicate and explain the potential impact: \"\n",
    "    \"`# TODO: this can impact performance` and `# REASON: [explanation]`\",\n",
    "    category=\"Optimization\",\n",
    ")\n",
    "_, memory_guideline = store_manager.create(create_memory_guideline_task)\n",
    "pprint_model(memory_guideline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e1fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '866d4fc1-a928-4a1e-bde5-1ab379494718',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Prefer Polars Lazy Evaluation',\n",
      " 'description': 'Prefer using Polars lazy evaluation in Python ETL processes '\n",
      "                'to optimize performance. Lazy evaluation allows Polars to '\n",
      "                'build and optimize a query execution plan before execution, '\n",
      "                'leading to more efficient operations and reduced memory '\n",
      "                'usage. Use [...]',\n",
      " 'category': 'Optimization',\n",
      " 'examples': None,\n",
      " 'embedding': [-0.020178577, 0.03625054, 0.061395004]}\n"
     ]
    }
   ],
   "source": [
    "create_polars_lazy_guideline_task = CreateGuidelineTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Prefer Polars Lazy Evaluation\",\n",
    "    description=\"Prefer using Polars lazy evaluation in Python ETL processes to \"\n",
    "    \"optimize performance. Lazy evaluation allows Polars to build and optimize a query \"\n",
    "    \"execution plan before execution, leading to more efficient operations and reduced \"\n",
    "    \"memory usage. Use lazy DataFrames (e.g., via `pl.lazy()` or `scan_csv()`) instead \"\n",
    "    \"of eager DataFrames where possible. Collect results only when necessary using \"\n",
    "    \"`.collect()`. This approach can significantly improve performance, especially for \"\n",
    "    \"complex queries involving multiple operations.\",\n",
    "    category=\"Optimization\",\n",
    ")\n",
    "_, polars_lazy_guideline = store_manager.create(create_polars_lazy_guideline_task)\n",
    "pprint_model(polars_lazy_guideline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf31579",
   "metadata": {},
   "source": [
    "## Define Project Structure\n",
    "\n",
    "Set up the standard source code structure for a Python ETL application including:\n",
    "\n",
    "- Root directory and package initialization\n",
    "- Main entry point for the ETL process\n",
    "- Separate modules for extract, transform, and load operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe513f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'c1201a09-a3d6-4ef9-8247-6d61a6f09b59',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo',\n",
      " 'description': 'Root source directory for Python application',\n",
      " 'structure_type': 'directory',\n",
      " 'embedding': [-0.04562018, -0.018908845, 0.032933872]}\n"
     ]
    }
   ],
   "source": [
    "create_src_root_task = CreateSrcStructureTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    path=\"src/foo\",\n",
    "    description=\"Root source directory for Python application\",\n",
    "    structure_type=\"directory\",\n",
    ")\n",
    "_, src_root_structure = store_manager.create(create_src_root_task)\n",
    "pprint_model(src_root_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91a1cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '6afcedb7-7ce0-468d-93e6-222757aa54c6',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/__init__.py',\n",
      " 'description': 'Package initialization file',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [-0.036667243, -0.014609963, 0.014548799]}\n"
     ]
    }
   ],
   "source": [
    "create_src_init_task = CreateSrcStructureTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    path=\"src/foo/__init__.py\",\n",
    "    description=\"Package initialization file\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "_, src_init_structure = store_manager.create(create_src_init_task)\n",
    "pprint_model(src_init_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d2d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '7c2103f1-5949-4d95-88e0-1d984dc84d99',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/__main__.py',\n",
      " 'description': 'Main entry point for the ETL script',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [-0.018054372, 0.0037061085, 0.010968876]}\n"
     ]
    }
   ],
   "source": [
    "create_src_main_task = CreateSrcStructureTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    path=\"src/foo/__main__.py\",\n",
    "    description=\"Main entry point for the ETL script\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "_, src_main_structure = store_manager.create(create_src_main_task)\n",
    "pprint_model(src_main_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03680ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '36492afb-c722-4932-b63d-e643ed15b6cc',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/extract.py',\n",
      " 'description': 'Data extraction module',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [-0.016554547, -0.06822745, -0.016024876]}\n"
     ]
    }
   ],
   "source": [
    "create_src_extract_task = CreateSrcStructureTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    path=\"src/foo/extract.py\",\n",
    "    description=\"Data extraction module\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "_, src_extract_structure = store_manager.create(create_src_extract_task)\n",
    "pprint_model(src_extract_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b31d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '892d253c-01d3-439c-864d-27990ab2afbb',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/transform.py',\n",
      " 'description': 'Data transformation module',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [0.0045563127, -0.074894615, -0.011505007]}\n"
     ]
    }
   ],
   "source": [
    "create_src_transform_task = CreateSrcStructureTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    path=\"src/foo/transform.py\",\n",
    "    description=\"Data transformation module\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "_, src_transform_structure = store_manager.create(create_src_transform_task)\n",
    "pprint_model(src_transform_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d307617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'f5a03e67-992d-478a-8f47-ccaba8758e2e',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/load.py',\n",
      " 'description': 'Data loading module',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [0.00481641, -0.043022912, 0.008896195]}\n"
     ]
    }
   ],
   "source": [
    "create_src_load_task = CreateSrcStructureTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    path=\"src/foo/load.py\",\n",
    "    description=\"Data loading module\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "_, src_load_structure = store_manager.create(create_src_load_task)\n",
    "pprint_model(src_load_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b889ce4",
   "metadata": {},
   "source": [
    "## Define Design Patterns and Common Abstractions\n",
    "\n",
    "Establish reusable abstractions and design patterns useful in ETL processing including:\n",
    "\n",
    "- Common design patterns (Adapter, Mediator, Facade, Singleton)\n",
    "- Data processing patterns (Data Enricher, Data Router, Dataset Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f9185bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'b352bbed-fb48-4d2c-aca3-4188d5fc76c6',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Adapter',\n",
      " 'description': 'Structural design pattern that allows objects with '\n",
      "                'incompatible interfaces to collaborate',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Adapter is a structural design pattern that allows objects with '\n",
      "            'incompatible interfaces to collaborate. An adapter wraps one of '\n",
      "            'the objects to hide the complexity of conversion happening behind '\n",
      "            'the scenes.',\n",
      " 'tags': ['structural', 'compatibility', 'interface'],\n",
      " 'embedding': [0.0074661355, -0.008026236, -0.019360468]}\n"
     ]
    }
   ],
   "source": [
    "create_adapter_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Adapter\",\n",
    "    description=\"Structural design pattern that allows objects with incompatible \"\n",
    "    \"interfaces to collaborate\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Adapter is a structural design pattern that allows objects with \"\n",
    "    \"incompatible interfaces to collaborate. An adapter wraps one of the objects to \"\n",
    "    \"hide the complexity of conversion happening behind the scenes.\",\n",
    "    tags=[\"structural\", \"compatibility\", \"interface\"],\n",
    ")\n",
    "_, adapter_abstraction = store_manager.create(create_adapter_task)\n",
    "pprint_model(adapter_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bb10a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '6053b70b-e6a2-42c8-b19d-82f24a9317f9',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Mediator',\n",
      " 'description': 'Behavioral design pattern that reduces chaotic dependencies '\n",
      "                'between objects by forcing collaboration through a mediator',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Mediator is a behavioral design pattern that lets you reduce '\n",
      "            'chaotic dependencies between objects. The pattern restricts '\n",
      "            'direct communications between the objects and forces them to '\n",
      "            'collaborate only via a mediator object.',\n",
      " 'tags': ['behavioral', 'decoupling', 'communication'],\n",
      " 'embedding': [-0.015068922, -0.06496549, 0.020649796]}\n"
     ]
    }
   ],
   "source": [
    "create_mediator_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Mediator\",\n",
    "    description=\"Behavioral design pattern that reduces chaotic dependencies between \"\n",
    "    \"objects by forcing collaboration through a mediator\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Mediator is a behavioral design pattern that lets you reduce chaotic \"\n",
    "    \"dependencies between objects. The pattern restricts direct communications between \"\n",
    "    \"the objects and forces them to collaborate only via a mediator object.\",\n",
    "    tags=[\"behavioral\", \"decoupling\", \"communication\"],\n",
    ")\n",
    "_, mediator_abstraction = store_manager.create(create_mediator_task)\n",
    "pprint_model(mediator_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c4a194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '7c656b3e-2a97-4cbe-a466-18960b0fa110',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Facade',\n",
      " 'description': 'Structural design pattern that provides a simplified '\n",
      "                'interface to a complex subsystem',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Facade is a structural design pattern that provides a simplified '\n",
      "            'interface to a library, a framework, or any other complex set of '\n",
      "            'classes. A facade provides limited functionality in comparison to '\n",
      "            'working with the subsystem directly.',\n",
      " 'tags': ['structural', 'simplification', 'interface'],\n",
      " 'embedding': [-0.0037615807, -0.013434297, -0.030911405]}\n"
     ]
    }
   ],
   "source": [
    "create_facade_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Facade\",\n",
    "    description=\"Structural design pattern that provides a simplified interface to a \"\n",
    "    \"complex subsystem\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Facade is a structural design pattern that provides a simplified \"\n",
    "    \"interface to a library, a framework, or any other complex set of classes. A \"\n",
    "    \"facade provides limited functionality in comparison to working with the subsystem \"\n",
    "    \"directly.\",\n",
    "    tags=[\"structural\", \"simplification\", \"interface\"],\n",
    ")\n",
    "_, facade_abstraction = store_manager.create(create_facade_task)\n",
    "pprint_model(facade_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a8c4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '3a262dc8-67bc-4285-a008-5ca3f2d7ef9e',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Singleton',\n",
      " 'description': 'Creational design pattern that ensures a class has only one '\n",
      "                'instance with global access',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Singleton is a creational design pattern that lets you ensure '\n",
      "            'that a class has only one instance, while providing a global '\n",
      "            'access point to this instance. Make the default constructor '\n",
      "            'private and create a static creation method that acts as a co '\n",
      "            '[...]',\n",
      " 'tags': ['creational', 'instance', 'global'],\n",
      " 'embedding': [0.0033135426, 0.0075293877, 0.005304555]}\n"
     ]
    }
   ],
   "source": [
    "create_singleton_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Singleton\",\n",
    "    description=\"Creational design pattern that ensures a class has only one instance \"\n",
    "    \"with global access\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Singleton is a creational design pattern that lets you ensure that a \"\n",
    "    \"class has only one instance, while providing a global access point to this \"\n",
    "    \"instance. Make the default constructor private and create a static creation \"\n",
    "    \"method that acts as a constructor.\",\n",
    "    tags=[\"creational\", \"instance\", \"global\"],\n",
    ")\n",
    "_, singleton_abstraction = store_manager.create(create_singleton_task)\n",
    "pprint_model(singleton_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f738ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '224e7a44-8a5b-43aa-aeb8-5cff6de40f0d',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Data Enricher',\n",
      " 'description': 'Uses information from the incoming dataset to retrieve '\n",
      "                'additional data from an external source and appends it to the '\n",
      "                'dataset',\n",
      " 'abstraction_type': 'data processing pattern',\n",
      " 'content': 'The Data Enricher uses information from the incoming dataset '\n",
      "            '(e.g., key fields) to retrieve data from an external source. '\n",
      "            'After retrieval, it appends the additional data to the dataset.',\n",
      " 'tags': ['transformation', 'enrichment', 'data-processing'],\n",
      " 'embedding': [-0.031380683, -0.016995514, -0.035144974]}\n"
     ]
    }
   ],
   "source": [
    "create_data_enricher_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Data Enricher\",\n",
    "    description=\"Uses information from the incoming dataset to retrieve additional \"\n",
    "    \"data from an external source and appends it to the dataset\",\n",
    "    abstraction_type=\"data processing pattern\",\n",
    "    content=\"The Data Enricher uses information from the incoming dataset \"\n",
    "    \"(e.g., key fields) to retrieve data from an external source. After retrieval, \"\n",
    "    \"it appends the additional data to the dataset.\",\n",
    "    tags=[\"transformation\", \"enrichment\", \"data-processing\"],\n",
    ")\n",
    "_, data_enricher_abstraction = store_manager.create(create_data_enricher_task)\n",
    "pprint_model(data_enricher_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e50b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '9322ad9f-b9b4-454a-80ab-70252f00ca64',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Data Router',\n",
      " 'description': 'Examines dataset content and routes data to different '\n",
      "                'processing paths based on field values or structure',\n",
      " 'abstraction_type': 'data processing pattern',\n",
      " 'content': 'The Data Router examines the dataset content and routes elements '\n",
      "            'to different processing paths based on criteria such as field '\n",
      "            'values, existence of fields, or data quality checks.',\n",
      " 'tags': ['routing', 'dataflow', 'data-processing'],\n",
      " 'embedding': [-0.009200101, -0.052700326, 0.024571396]}\n"
     ]
    }
   ],
   "source": [
    "create_data_router_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Data Router\",\n",
    "    description=\"Examines dataset content and routes data to different processing \"\n",
    "    \"paths based on field values or structure\",\n",
    "    abstraction_type=\"data processing pattern\",\n",
    "    content=\"The Data Router examines the dataset content and routes elements to \"\n",
    "    \"different processing paths based on criteria such as field values, \"\n",
    "    \"existence of fields, or data quality checks.\",\n",
    "    tags=[\"routing\", \"dataflow\", \"data-processing\"],\n",
    ")\n",
    "_, data_router_abstraction = store_manager.create(create_data_router_task)\n",
    "pprint_model(data_router_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49b368d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'ed1cfc11-bf15-4451-939a-c450eab4374b',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Dataset Splitter',\n",
      " 'description': 'Breaks composite datasets into smaller subsets or individual '\n",
      "                'records for separate processing',\n",
      " 'abstraction_type': 'data processing pattern',\n",
      " 'content': 'The Dataset Splitter breaks composite datasets (e.g., nested '\n",
      "            'records or batched data) into smaller subsets or individual '\n",
      "            'records for distributed or sequential processing.',\n",
      " 'tags': ['decomposition', 'data-processing', 'transformation'],\n",
      " 'embedding': [-0.02178357, -0.04705736, -0.028762724]}\n"
     ]
    }
   ],
   "source": [
    "create_splitter_task = CreateAbstractionTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Dataset Splitter\",\n",
    "    description=\"Breaks composite datasets into smaller subsets or individual records \"\n",
    "    \"for separate processing\",\n",
    "    abstraction_type=\"data processing pattern\",\n",
    "    content=\"The Dataset Splitter breaks composite datasets (e.g., nested records or \"\n",
    "    \"batched data) into smaller subsets or individual records for distributed or \"\n",
    "    \"sequential processing.\",\n",
    "    tags=[\"decomposition\", \"data-processing\", \"transformation\"],\n",
    ")\n",
    "_, splitter_abstraction = store_manager.create(create_splitter_task)\n",
    "pprint_model(splitter_abstraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632a37c",
   "metadata": {},
   "source": [
    "## ETL Functions\n",
    "\n",
    "Provides concrete code implementation examples for key ETL operations including:\n",
    "\n",
    "- Extract function to fetch data\n",
    "- Transform function to process data using Polars DataFrame\n",
    "- Load function to store processed data in SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95e4eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'e9121dca-c3e2-429a-94f3-d3694fa10902',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Extract Function',\n",
      " 'description': 'Fetches data from the English Wikipedia API for a given page '\n",
      "                'title and returns a dictionary with the page title and '\n",
      "                'extract text.',\n",
      " 'content': 'def extract(page_title: str) -> dict[str, str]: [...]',\n",
      " 'tags': ['etl', 'extract', 'wikipedia'],\n",
      " 'embedding': [0.0044612205, 0.0029688897, 0.01244339]}\n"
     ]
    }
   ],
   "source": [
    "create_extract_task = CreateCodeTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Extract Function\",\n",
    "    description=\"Fetches data from the English Wikipedia API for a given page title \"\n",
    "    \"and returns a dictionary with the page title and extract text.\",\n",
    "    content='''def extract(page_title: str) -> dict[str, str]:\n",
    "    \"\"\"Extract data from the English Wikipedia API for a given page title.\n",
    "\n",
    "    Args:\n",
    "        page_title (str): The exact title of the Wikipedia page to fetch.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the page title and the plain text\n",
    "            extract.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": page_title,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return {\"title\": page.get(\"title\", \"\"), \"extract\": page.get(\"extract\", \"\")}''',\n",
    "    tags=[\"etl\", \"extract\", \"wikipedia\", \"requests\"],\n",
    ")\n",
    "_, extract_code = store_manager.create(create_extract_task)\n",
    "pprint_model(extract_code, fields_to_clip_at_first_line=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff979a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '73a55f03-991b-460b-bcae-dbed9fab6e5e',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Transform Function',\n",
      " 'description': 'Transforms extracted Wikipedia data into a Polars DataFrame '\n",
      "                'and adds a column with the text length of each extract.',\n",
      " 'content': 'def transform(data: list[dict[str, str]]) -> pl.DataFrame: [...]',\n",
      " 'tags': ['etl', 'transform', 'polars'],\n",
      " 'embedding': [-0.02371679, -0.0022043292, -0.004579774]}\n"
     ]
    }
   ],
   "source": [
    "create_transform_task = CreateCodeTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Transform Function\",\n",
    "    description=\"Transforms extracted Wikipedia data into a Polars DataFrame and \"\n",
    "    \"adds a column with the text length of each extract.\",\n",
    "    content='''def transform(data: list[dict[str, str]]) -> pl.DataFrame:\n",
    "    \"\"\"Transform extracted Wikipedia data into a Polars DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (list[dict[str, str]]): A list of dictionaries with keys 'title' and\n",
    "            'extract'.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A Polars DataFrame containing page titles and their text lengths.\n",
    "    \"\"\"\n",
    "    df = pl.DataFrame(data)\n",
    "    return df.with_columns([\n",
    "        pl.col(\"extract\").str.len_chars().alias(\"text_length\")\n",
    "    ])''',\n",
    "    tags=[\"etl\", \"transform\", \"polars\", \"dataframe\"],\n",
    ")\n",
    "_, transform_code = store_manager.create(create_transform_task)\n",
    "pprint_model(transform_code, fields_to_clip_at_first_line=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78d67697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'e9b19024-d070-411e-8b83-2645e5c3b5af',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Load Function',\n",
      " 'description': 'Loads transformed Wikipedia data from a Polars DataFrame into '\n",
      "                \"an SQLite database table named 'wikipedia'.\",\n",
      " 'content': 'def load(df: pl.DataFrame, db_path: str = \"wikipedia.db\") -> '\n",
      "            'None: [...]',\n",
      " 'tags': ['etl', 'load', 'sqlite'],\n",
      " 'embedding': [0.035206925, 0.047198765, -0.023641724]}\n"
     ]
    }
   ],
   "source": [
    "create_load_task = CreateCodeTask(\n",
    "    language_context_key=lang_context_key,\n",
    "    name=\"Load Function\",\n",
    "    description=\"Loads transformed Wikipedia data from a Polars DataFrame into an \"\n",
    "    \"SQLite database table named 'wikipedia'.\",\n",
    "    content='''def load(df: pl.DataFrame, db_path: str = \"wikipedia.db\") -> None:\n",
    "    \"\"\"Load transformed data into an SQLite database.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The Polars DataFrame containing Wikipedia data.\n",
    "        db_path (str, optional): Path to the SQLite database file. Defaults to\n",
    "            'wikipedia.db'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS wikipedia (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            extract TEXT,\n",
    "            text_length INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.executemany(\n",
    "        \"INSERT INTO wikipedia (title, extract, text_length) VALUES (?, ?, ?)\",\n",
    "        [(row[\"title\"], row[\"extract\"], row[\"text_length\"]) for row in df.to_dicts()]\n",
    "    )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()''',\n",
    "    tags=[\"etl\", \"load\", \"sqlite\", \"database\"],\n",
    ")\n",
    "_, load_code = store_manager.create(create_load_task)\n",
    "pprint_model(load_code, fields_to_clip_at_first_line=[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
