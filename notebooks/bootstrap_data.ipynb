{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab26265",
   "metadata": {},
   "source": [
    "# BlueprintFlow: Bootstrap Example\n",
    "\n",
    "This notebook serves as an example demonstrating how to systematically bootstrap data into BlueprintFlow for demonstration purposes.\n",
    "\n",
    "## What You'll Bootstrap\n",
    "\n",
    "- **Language Context**: A language context for Python ETL development\n",
    "- **Coding Standards**: Rules that must be followed in the codebase and guidelines and coding standards for Python ETL development\n",
    "- **Project Structure**: Source code structure for an ETL project\n",
    "- **Design Patterns**: Common abstractions and design patterns useful in ETL processes\n",
    "- **Tool Stack**: Preferred libraries and tools for ETL processes\n",
    "- **Implementation Examples**: Example code implementations for extract, transform, and load operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9f213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49d44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb_util import pprint_lancedb_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339e3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blueprintflow.core.models.data_store import TableNameEnum\n",
    "from blueprintflow.core.models.tasks import (\n",
    "    CreateAstractionTask,\n",
    "    CreateCodeTask,\n",
    "    CreateGuidelineTask,\n",
    "    CreateLanguageContextTask,\n",
    "    CreatePreferenceTask,\n",
    "    CreateRuleTask,\n",
    "    CreateSrcStructureTask,\n",
    ")\n",
    "from blueprintflow.core.settings import load_settings\n",
    "from blueprintflow.store.store_manager import StoreManager\n",
    "from blueprintflow.utils.xdg.data import UserData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d5465",
   "metadata": {},
   "source": [
    "##### Initialize BlueprintFlow Components\n",
    "\n",
    "This section sets up the necessary components for interacting with BlueprintFlow's database:\n",
    "- Loads default settings\n",
    "- Initializes the default user data storage\n",
    "- Creates a store manager to handle database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d7ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_settings = load_settings()\n",
    "default_user_data = UserData(reset_if_exists=True)\n",
    "store_manager = StoreManager(default_settings, default_user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf686c",
   "metadata": {},
   "source": [
    "##### Define Language Context for Python ETL\n",
    "\n",
    "Establishes the core language context for the blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a509cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl',\n",
      " 'language': 'python',\n",
      " 'context': 'etl',\n",
      " 'description': 'Python is a versatile and readable language ideal for ETL '\n",
      "                'processes. It excels in handling data extraction from diverse '\n",
      "                'sources, transforming data into required formats, and loading '\n",
      "                'it into target systems. Its flexibility and extensive '\n",
      "                'ecosyste [...]',\n",
      " 'embedding': [-0.00034710637, -0.02914067, 0.055726133]}\n"
     ]
    }
   ],
   "source": [
    "lang_context_key = \"python_etl\"\n",
    "create_lang_context_task = CreateLanguageContextTask(\n",
    "    key=lang_context_key,\n",
    "    language=\"python\",\n",
    "    context=\"etl\",\n",
    "    description=\"Python is a versatile and readable language ideal for ETL processes. \"\n",
    "    \"It excels in handling data extraction from diverse sources, transforming data \"\n",
    "    \"into required formats, and loading it into target systems. \"\n",
    "    \"Its flexibility and extensive ecosystem make it a popular choice for building \"\n",
    "    \"efficient data processing workflows and integration tasks.\",\n",
    ")\n",
    "store_manager.create(create_lang_context_task)\n",
    "lang_context = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.LANG_CONTEXT, lang_context_key\n",
    ")\n",
    "pprint_lancedb_record(lang_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e092b4a",
   "metadata": {},
   "source": [
    "##### Define Rules and Standards\n",
    "\n",
    "Establishes strict coding rules that must be followed in the ETL implementation including:\n",
    "\n",
    "- Documentation standards (Google-style docstrings)\n",
    "- Type safety requirements (static typing for functions)\n",
    "- Code quality rules (no assert statements outside tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b439f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_google_docstrings',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Google convention docstrings',\n",
      " 'description': 'Always generate docstrings following the Google convention',\n",
      " 'rule_type': 'documentation',\n",
      " 'violations_action': 'require_fix',\n",
      " 'embedding': [-0.058820747, -0.042512137, -0.018018302]}\n"
     ]
    }
   ],
   "source": [
    "docstring_rule_key = \"python_etl_google_docstrings\"\n",
    "create_docstring_rule_task = CreateRuleTask(\n",
    "    key=docstring_rule_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Google convention docstrings\",\n",
    "    description=\"Always generate docstrings following the Google convention\",\n",
    "    rule_type=\"documentation\",\n",
    "    violations_action=\"require_fix\",\n",
    ")\n",
    "store_manager.create(create_docstring_rule_task)\n",
    "docstring_rule = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.RULE, docstring_rule_key\n",
    ")\n",
    "pprint_lancedb_record(docstring_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c2d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_static_typing',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Static typing for functions',\n",
      " 'description': 'Always generate static typing for function definitions to be '\n",
      "                'evaluated with mypy later on',\n",
      " 'rule_type': 'typing',\n",
      " 'violations_action': 'require_fix',\n",
      " 'embedding': [-0.026746703, 0.0070767906, 0.05300787]}\n"
     ]
    }
   ],
   "source": [
    "typing_rule_key = \"python_etl_static_typing\"\n",
    "create_typing_rule_task = CreateRuleTask(\n",
    "    key=typing_rule_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Static typing for functions\",\n",
    "    description=\"Always generate static typing for function definitions to be \"\n",
    "    \"evaluated with mypy later on\",\n",
    "    rule_type=\"typing\",\n",
    "    violations_action=\"require_fix\",\n",
    ")\n",
    "store_manager.create(create_typing_rule_task)\n",
    "typing_rule = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.RULE, typing_rule_key\n",
    ")\n",
    "pprint_lancedb_record(typing_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e900a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_no_assert_outside_tests',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'No assert outside tests',\n",
      " 'description': 'Do not use assert outside of test functions',\n",
      " 'rule_type': 'style',\n",
      " 'violations_action': 'require_fix',\n",
      " 'embedding': [-0.010566957, -0.016409732, 0.036240608]}\n"
     ]
    }
   ],
   "source": [
    "assert_rule_key = \"python_etl_no_assert_outside_tests\"\n",
    "create_assert_rule_task = CreateRuleTask(\n",
    "    key=assert_rule_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"No assert outside tests\",\n",
    "    description=\"Do not use assert outside of test functions\",\n",
    "    rule_type=\"style\",\n",
    "    violations_action=\"require_fix\",\n",
    ")\n",
    "store_manager.create(create_assert_rule_task)\n",
    "assert_rule = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.RULE, assert_rule_key\n",
    ")\n",
    "pprint_lancedb_record(assert_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7050f",
   "metadata": {},
   "source": [
    "##### Establish Guidelines\n",
    "\n",
    "Defines coding guidelines and best practices for Python ETL development including:\n",
    "\n",
    "- The Zen of Python principles\n",
    "- Style guidelines like proper boolean evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c951e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_zen_of_python',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'The Zen of Python',\n",
      " 'description': 'Beautiful is better than ugly.\\n'\n",
      "                'Explicit is better than implicit.\\n'\n",
      "                'Simple is better than complex.\\n'\n",
      "                'Complex is better than complicated.\\n'\n",
      "                'Flat is better than nested.\\n'\n",
      "                'Sparse is better than dense.\\n'\n",
      "                'Readability counts.\\n'\n",
      "                \"Special cases aren't special enough [...]\",\n",
      " 'category': 'principles',\n",
      " 'examples': None,\n",
      " 'embedding': [-0.07254717, -0.003949336, 0.05075067]}\n"
     ]
    }
   ],
   "source": [
    "zen_of_python_key = \"python_etl_zen_of_python\"\n",
    "create_zen_guideline_task = CreateGuidelineTask(\n",
    "    key=zen_of_python_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"The Zen of Python\",\n",
    "    description=inspect.cleandoc(\n",
    "        \"\"\"\n",
    "        Beautiful is better than ugly.\n",
    "        Explicit is better than implicit.\n",
    "        Simple is better than complex.\n",
    "        Complex is better than complicated.\n",
    "        Flat is better than nested.\n",
    "        Sparse is better than dense.\n",
    "        Readability counts.\n",
    "        Special cases aren't special enough to break the rules.\n",
    "        Although practicality beats purity.\n",
    "        Errors should never pass silently.\n",
    "        Unless explicitly silenced.\n",
    "        In the face of ambiguity, refuse the temptation to guess.\n",
    "        There should be one-- and preferably only one --obvious way to do it.\n",
    "        Although that way may not be obvious at first unless you're Dutch.\n",
    "        Now is better than never.\n",
    "        Although never is often better than *right* now.\n",
    "        If the implementation is hard to explain, it's a bad idea.\n",
    "        If the implementation is easy to explain, it may be a good idea.\n",
    "        Namespaces are one honking great idea -- let's do more of those!\"\"\"\n",
    "    ),\n",
    "    category=\"principles\",\n",
    ")\n",
    "store_manager.create(create_zen_guideline_task)\n",
    "zen_guideline = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.GUIDELINE, zen_of_python_key\n",
    ")\n",
    "pprint_lancedb_record(zen_guideline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c65acc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_bool_evaluation',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Variable bool evaluation',\n",
      " 'description': \"Use Python's truthiness directly instead of explicit \"\n",
      "                'comparisons to True and False',\n",
      " 'category': 'style',\n",
      " 'examples': ['if attr: # Good - check truthiness',\n",
      "              'if not attr: # Good - check falsiness',\n",
      "              'if attr is None: # Good - explicit None check'],\n",
      " 'embedding': [-0.052617468, 0.01749514, 0.062484514]}\n"
     ]
    }
   ],
   "source": [
    "bool_evaluation_key = \"python_etl_bool_evaluation\"\n",
    "create_bool_evaluation_task = CreateGuidelineTask(\n",
    "    key=bool_evaluation_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Variable bool evaluation\",\n",
    "    description=\"Use Python's truthiness directly instead of explicit comparisons to \"\n",
    "    \"True and False\",\n",
    "    category=\"style\",\n",
    "    examples=[\n",
    "        \"if attr: # Good - check truthiness\",\n",
    "        \"if not attr: # Good - check falsiness\",\n",
    "        \"if attr is None: # Good - explicit None check\",\n",
    "        \"if attr == True: # Bad - unnecessary comparison\",\n",
    "    ],\n",
    ")\n",
    "store_manager.create(create_bool_evaluation_task)\n",
    "bool_evaluation_guideline = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.GUIDELINE, bool_evaluation_key\n",
    ")\n",
    "pprint_lancedb_record(bool_evaluation_guideline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf31579",
   "metadata": {},
   "source": [
    "##### Define Project Structure\n",
    "\n",
    "Sets up the standard source code structure for a Python ETL application including:\n",
    "\n",
    "- Root directory and package initialization\n",
    "- Main entry point for the ETL process\n",
    "- Separate modules for extract, transform, and load operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe513f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_src_root',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo',\n",
      " 'description': 'Root source directory for Python application',\n",
      " 'structure_type': 'directory',\n",
      " 'embedding': [-0.04562018, -0.018908845, 0.032933872]}\n"
     ]
    }
   ],
   "source": [
    "src_root_key = \"python_etl_src_root\"\n",
    "create_src_root_task = CreateSrcStructureTask(\n",
    "    key=src_root_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    path=\"src/foo\",\n",
    "    description=\"Root source directory for Python application\",\n",
    "    structure_type=\"directory\",\n",
    ")\n",
    "store_manager.create(create_src_root_task)\n",
    "src_root_structure = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.SRC_STRUCTURE, src_root_key\n",
    ")\n",
    "pprint_lancedb_record(src_root_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c91a1cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_src_init',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/__init__.py',\n",
      " 'description': 'Package initialization file',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [-0.036667243, -0.014609963, 0.014548799]}\n"
     ]
    }
   ],
   "source": [
    "src_init_key = \"python_etl_src_init\"\n",
    "create_src_init_task = CreateSrcStructureTask(\n",
    "    key=src_init_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    path=\"src/foo/__init__.py\",\n",
    "    description=\"Package initialization file\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "store_manager.create(create_src_init_task)\n",
    "src_init_structure = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.SRC_STRUCTURE, src_init_key\n",
    ")\n",
    "pprint_lancedb_record(src_init_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d2d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_src_main',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/__main__.py',\n",
      " 'description': 'Main entry point for the ETL script',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [-0.018054372, 0.0037061085, 0.010968876]}\n"
     ]
    }
   ],
   "source": [
    "src_main_key = \"python_etl_src_main\"\n",
    "create_src_main_task = CreateSrcStructureTask(\n",
    "    key=src_main_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    path=\"src/foo/__main__.py\",\n",
    "    description=\"Main entry point for the ETL script\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "store_manager.create(create_src_main_task)\n",
    "src_main_structure = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.SRC_STRUCTURE, src_main_key\n",
    ")\n",
    "pprint_lancedb_record(src_main_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03680ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_src_extract',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/extract.py',\n",
      " 'description': 'Data extraction module',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [-0.016554547, -0.06822745, -0.016024876]}\n"
     ]
    }
   ],
   "source": [
    "src_extract_key = \"python_etl_src_extract\"\n",
    "create_src_extract_task = CreateSrcStructureTask(\n",
    "    key=src_extract_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    path=\"src/foo/extract.py\",\n",
    "    description=\"Data extraction module\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "store_manager.create(create_src_extract_task)\n",
    "src_extract_structure = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.SRC_STRUCTURE, src_extract_key\n",
    ")\n",
    "pprint_lancedb_record(src_extract_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b31d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_src_transform',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/transform.py',\n",
      " 'description': 'Data transformation module',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [0.0045563127, -0.074894615, -0.011505007]}\n"
     ]
    }
   ],
   "source": [
    "src_transform_key = \"python_etl_src_transform\"\n",
    "create_src_transform_task = CreateSrcStructureTask(\n",
    "    key=src_transform_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    path=\"src/foo/transform.py\",\n",
    "    description=\"Data transformation module\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "store_manager.create(create_src_transform_task)\n",
    "src_transform_structure = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.SRC_STRUCTURE, src_transform_key\n",
    ")\n",
    "pprint_lancedb_record(src_transform_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d307617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_src_load',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'path': 'src/foo/load.py',\n",
      " 'description': 'Data loading module',\n",
      " 'structure_type': 'file',\n",
      " 'embedding': [0.00481641, -0.043022912, 0.008896195]}\n"
     ]
    }
   ],
   "source": [
    "src_load_key = \"python_etl_src_load\"\n",
    "create_src_load_task = CreateSrcStructureTask(\n",
    "    key=src_load_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    path=\"src/foo/load.py\",\n",
    "    description=\"Data loading module\",\n",
    "    structure_type=\"file\",\n",
    ")\n",
    "store_manager.create(create_src_load_task)\n",
    "src_load_structure = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.SRC_STRUCTURE, src_load_key\n",
    ")\n",
    "pprint_lancedb_record(src_load_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b889ce4",
   "metadata": {},
   "source": [
    "##### Define Common Abstractions and Design Patterns\n",
    "\n",
    "Establishes reusable abstractions and design patterns useful in ETL processing including:\n",
    "\n",
    "- Common design patterns (Adapter, Mediator, Facade, Singleton)\n",
    "- Data processing patterns (Data Enricher, Data Router, Dataset Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9185bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_adapter',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Adapter',\n",
      " 'description': 'Structural design pattern that allows objects with '\n",
      "                'incompatible interfaces to collaborate',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Adapter is a structural design pattern that allows objects with '\n",
      "            'incompatible interfaces to collaborate. An adapter wraps one of '\n",
      "            'the objects to hide the complexity of conversion happening behind '\n",
      "            'the scenes.',\n",
      " 'tags': ['structural', 'compatibility', 'interface'],\n",
      " 'embedding': [0.0074661355, -0.008026236, -0.019360468]}\n"
     ]
    }
   ],
   "source": [
    "adapter_key = \"python_etl_adapter\"\n",
    "create_adapter_task = CreateAstractionTask(\n",
    "    key=adapter_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Adapter\",\n",
    "    description=\"Structural design pattern that allows objects with incompatible \"\n",
    "    \"interfaces to collaborate\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Adapter is a structural design pattern that allows objects with \"\n",
    "    \"incompatible interfaces to collaborate. An adapter wraps one of the objects to \"\n",
    "    \"hide the complexity of conversion happening behind the scenes.\",\n",
    "    tags=[\"structural\", \"compatibility\", \"interface\"],\n",
    ")\n",
    "store_manager.create(create_adapter_task)\n",
    "adapter_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, adapter_key\n",
    ")\n",
    "pprint_lancedb_record(adapter_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb10a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_mediator',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Mediator',\n",
      " 'description': 'Behavioral design pattern that reduces chaotic dependencies '\n",
      "                'between objects by forcing collaboration through a mediator',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Mediator is a behavioral design pattern that lets you reduce '\n",
      "            'chaotic dependencies between objects. The pattern restricts '\n",
      "            'direct communications between the objects and forces them to '\n",
      "            'collaborate only via a mediator object.',\n",
      " 'tags': ['behavioral', 'decoupling', 'communication'],\n",
      " 'embedding': [-0.015068922, -0.06496549, 0.020649796]}\n"
     ]
    }
   ],
   "source": [
    "mediator_key = \"python_etl_mediator\"\n",
    "create_mediator_task = CreateAstractionTask(\n",
    "    key=mediator_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Mediator\",\n",
    "    description=\"Behavioral design pattern that reduces chaotic dependencies between \"\n",
    "    \"objects by forcing collaboration through a mediator\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Mediator is a behavioral design pattern that lets you reduce chaotic \"\n",
    "    \"dependencies between objects. The pattern restricts direct communications between \"\n",
    "    \"the objects and forces them to collaborate only via a mediator object.\",\n",
    "    tags=[\"behavioral\", \"decoupling\", \"communication\"],\n",
    ")\n",
    "store_manager.create(create_mediator_task)\n",
    "mediator_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, mediator_key\n",
    ")\n",
    "pprint_lancedb_record(mediator_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c4a194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_facade',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Facade',\n",
      " 'description': 'Structural design pattern that provides a simplified '\n",
      "                'interface to a complex subsystem',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Facade is a structural design pattern that provides a simplified '\n",
      "            'interface to a library, a framework, or any other complex set of '\n",
      "            'classes. A facade provides limited functionality in comparison to '\n",
      "            'working with the subsystem directly.',\n",
      " 'tags': ['structural', 'simplification', 'interface'],\n",
      " 'embedding': [-0.0037615807, -0.013434297, -0.030911405]}\n"
     ]
    }
   ],
   "source": [
    "facade_key = \"python_etl_facade\"\n",
    "create_facade_task = CreateAstractionTask(\n",
    "    key=facade_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Facade\",\n",
    "    description=\"Structural design pattern that provides a simplified interface to a \"\n",
    "    \"complex subsystem\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Facade is a structural design pattern that provides a simplified \"\n",
    "    \"interface to a library, a framework, or any other complex set of classes. A \"\n",
    "    \"facade provides limited functionality in comparison to working with the subsystem \"\n",
    "    \"directly.\",\n",
    "    tags=[\"structural\", \"simplification\", \"interface\"],\n",
    ")\n",
    "store_manager.create(create_facade_task)\n",
    "facade_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, facade_key\n",
    ")\n",
    "pprint_lancedb_record(facade_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a8c4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_singleton',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Singleton',\n",
      " 'description': 'Creational design pattern that ensures a class has only one '\n",
      "                'instance with global access',\n",
      " 'abstraction_type': 'design pattern',\n",
      " 'content': 'Singleton is a creational design pattern that lets you ensure '\n",
      "            'that a class has only one instance, while providing a global '\n",
      "            'access point to this instance. Make the default constructor '\n",
      "            'private and create a static creation method that acts as a co '\n",
      "            '[...]',\n",
      " 'tags': ['creational', 'instance', 'global'],\n",
      " 'embedding': [0.0033135426, 0.0075293877, 0.005304555]}\n"
     ]
    }
   ],
   "source": [
    "singleton_key = \"python_etl_singleton\"\n",
    "create_singleton_task = CreateAstractionTask(\n",
    "    key=singleton_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Singleton\",\n",
    "    description=\"Creational design pattern that ensures a class has only one instance \"\n",
    "    \"with global access\",\n",
    "    abstraction_type=\"design pattern\",\n",
    "    content=\"Singleton is a creational design pattern that lets you ensure that a \"\n",
    "    \"class has only one instance, while providing a global access point to this \"\n",
    "    \"instance. Make the default constructor private and create a static creation \"\n",
    "    \"method that acts as a constructor.\",\n",
    "    tags=[\"creational\", \"instance\", \"global\"],\n",
    ")\n",
    "store_manager.create(create_singleton_task)\n",
    "singleton_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, singleton_key\n",
    ")\n",
    "pprint_lancedb_record(singleton_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f738ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_data_enricher',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Data Enricher',\n",
      " 'description': 'Uses information from the incoming dataset to retrieve '\n",
      "                'additional data from an external source and appends it to the '\n",
      "                'dataset',\n",
      " 'abstraction_type': 'data processing pattern',\n",
      " 'content': 'The Data Enricher uses information from the incoming dataset '\n",
      "            '(e.g., key fields) to retrieve data from an external source. '\n",
      "            'After retrieval, it appends the additional data to the dataset.',\n",
      " 'tags': ['transformation', 'enrichment', 'data-processing'],\n",
      " 'embedding': [-0.031380683, -0.016995514, -0.035144974]}\n"
     ]
    }
   ],
   "source": [
    "data_enricher_key = \"python_etl_data_enricher\"\n",
    "create_data_enricher_task = CreateAstractionTask(\n",
    "    key=data_enricher_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Data Enricher\",\n",
    "    description=\"Uses information from the incoming dataset to retrieve additional \"\n",
    "    \"data from an external source and appends it to the dataset\",\n",
    "    abstraction_type=\"data processing pattern\",\n",
    "    content=\"The Data Enricher uses information from the incoming dataset \"\n",
    "    \"(e.g., key fields) to retrieve data from an external source. After retrieval, \"\n",
    "    \"it appends the additional data to the dataset.\",\n",
    "    tags=[\"transformation\", \"enrichment\", \"data-processing\"],\n",
    ")\n",
    "store_manager.create(create_data_enricher_task)\n",
    "data_enricher_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, data_enricher_key\n",
    ")\n",
    "pprint_lancedb_record(data_enricher_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e50b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_data_router',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Data Router',\n",
      " 'description': 'Examines dataset content and routes data to different '\n",
      "                'processing paths based on field values or structure',\n",
      " 'abstraction_type': 'data processing pattern',\n",
      " 'content': 'The Data Router examines the dataset content and routes elements '\n",
      "            'to different processing paths based on criteria such as field '\n",
      "            'values, existence of fields, or data quality checks.',\n",
      " 'tags': ['routing', 'dataflow', 'data-processing'],\n",
      " 'embedding': [-0.009200101, -0.052700326, 0.024571396]}\n"
     ]
    }
   ],
   "source": [
    "data_router_key = \"python_etl_data_router\"\n",
    "create_data_router_task = CreateAstractionTask(\n",
    "    key=data_router_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Data Router\",\n",
    "    description=\"Examines dataset content and routes data to different processing \"\n",
    "    \"paths based on field values or structure\",\n",
    "    abstraction_type=\"data processing pattern\",\n",
    "    content=\"The Data Router examines the dataset content and routes elements to \"\n",
    "    \"different processing paths based on criteria such as field values, \"\n",
    "    \"existence of fields, or data quality checks.\",\n",
    "    tags=[\"routing\", \"dataflow\", \"data-processing\"],\n",
    ")\n",
    "store_manager.create(create_data_router_task)\n",
    "data_router_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, data_router_key\n",
    ")\n",
    "pprint_lancedb_record(data_router_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b368d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_splitter',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Dataset Splitter',\n",
      " 'description': 'Breaks composite datasets into smaller subsets or individual '\n",
      "                'records for separate processing',\n",
      " 'abstraction_type': 'data processing pattern',\n",
      " 'content': 'The Dataset Splitter breaks composite datasets (e.g., nested '\n",
      "            'records or batched data) into smaller subsets or individual '\n",
      "            'records for distributed or sequential processing.',\n",
      " 'tags': ['decomposition', 'data-processing', 'transformation'],\n",
      " 'embedding': [-0.02178357, -0.04705736, -0.028762724]}\n"
     ]
    }
   ],
   "source": [
    "splitter_key = \"python_etl_splitter\"\n",
    "create_splitter_task = CreateAstractionTask(\n",
    "    key=splitter_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Dataset Splitter\",\n",
    "    description=\"Breaks composite datasets into smaller subsets or individual records \"\n",
    "    \"for separate processing\",\n",
    "    abstraction_type=\"data processing pattern\",\n",
    "    content=\"The Dataset Splitter breaks composite datasets (e.g., nested records or \"\n",
    "    \"batched data) into smaller subsets or individual records for distributed or \"\n",
    "    \"sequential processing.\",\n",
    "    tags=[\"decomposition\", \"data-processing\", \"transformation\"],\n",
    ")\n",
    "store_manager.create(create_splitter_task)\n",
    "splitter_abstraction = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.ABSTRACTION, splitter_key\n",
    ")\n",
    "pprint_lancedb_record(splitter_abstraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709f10b",
   "metadata": {},
   "source": [
    "##### Define Preferred Libraries for ETL\n",
    "\n",
    "Specifies preferred libraries for ETL operations in Python:\n",
    "\n",
    "- Polars: A high-performance DataFrame library optimized for large datasets\n",
    "- DuckDB: An embedded analytical database with SQL interface and efficient data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97667f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_polars',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'polars',\n",
      " 'description': 'A fast DataFrame library for Python designed as a powerful '\n",
      "                'tool for ETL tasks. It excels in efficient data '\n",
      "                'transformation and loading, powered by Rust for high '\n",
      "                'performance with large datasets. Adopts an imperative '\n",
      "                'programming approach, ideal fo [...]',\n",
      " 'tags': ['dataframe', 'performance', 'big data'],\n",
      " 'embedding': [-0.020702168, 0.028024463, 0.0018910654]}\n"
     ]
    }
   ],
   "source": [
    "polars_key = \"python_etl_polars\"\n",
    "create_polars_preference_task = CreatePreferenceTask(\n",
    "    key=polars_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"polars\",\n",
    "    description=\"A fast DataFrame library for Python designed as a powerful tool for \"\n",
    "    \"ETL tasks. It excels in efficient data transformation and loading, powered by \"\n",
    "    \"Rust for high performance with large datasets. Adopts an imperative programming \"\n",
    "    \"approach, ideal for developers who prefer native data manipulation and control \"\n",
    "    \"within Python.\",\n",
    "    tags=[\"dataframe\", \"performance\", \"big data\", \"Rust\", \"imperative\"],\n",
    ")\n",
    "store_manager.create(create_polars_preference_task)\n",
    "polars_preference = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.PREFERENCE, polars_key\n",
    ")\n",
    "pprint_lancedb_record(polars_preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be1b4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_duckdb',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'duckdb',\n",
      " 'description': 'An embedded, in-memory analytical database with columnar '\n",
      "                'storage, optimized for fast analytical queries and seamless '\n",
      "                'Python integration. Excels in ETL by efficiently processing '\n",
      "                'diverse data formats such as CSV, Parquet, Delta, and JSON. '\n",
      "                'Its lig [...]',\n",
      " 'tags': ['database', 'columnar', 'sql'],\n",
      " 'embedding': [-0.04089327, -0.0075872457, 0.011690059]}\n"
     ]
    }
   ],
   "source": [
    "duckdb_key = \"python_etl_duckdb\"\n",
    "create_duckdb_preference_task = CreatePreferenceTask(\n",
    "    key=duckdb_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"duckdb\",\n",
    "    description=\"An embedded, in-memory analytical database with columnar storage, \"\n",
    "    \"optimized for fast analytical queries and seamless Python integration. Excels in \"\n",
    "    \"ETL by efficiently processing diverse data formats such as CSV, Parquet, Delta, \"\n",
    "    \"and JSON. Its lightweight design and high-speed batch processing make it ideal \"\n",
    "    \"for cost-effective, lightweight ETL workflows. Supports a fully declarative \"\n",
    "    \"SQL-based approach, enabling complex data operations using SQL syntax. Integrates \"\n",
    "    \"smoothly with other tools, making it versatile for comprehensive data \"\n",
    "    \"transformation and analysis.\",\n",
    "    tags=[\n",
    "        \"database\",\n",
    "        \"columnar\",\n",
    "        \"sql\",\n",
    "        \"json\",\n",
    "        \"ETL\",\n",
    "        \"performance\",\n",
    "        \"lightweight\",\n",
    "        \"declarative\",\n",
    "    ],\n",
    ")\n",
    "store_manager.create(create_duckdb_preference_task)\n",
    "duckdb_preference = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.PREFERENCE, duckdb_key\n",
    ")\n",
    "pprint_lancedb_record(duckdb_preference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632a37c",
   "metadata": {},
   "source": [
    "##### Implement Core ETL Functions\n",
    "\n",
    "Provides concrete code implementation examples for key ETL operations including:\n",
    "\n",
    "- Extract function to fetch data\n",
    "- Transform function to process data using Polars DataFrame\n",
    "- Load function to store processed data in SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e4eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_extract',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Extract Function',\n",
      " 'description': 'Fetches data from the English Wikipedia API for a given page '\n",
      "                'title and returns a dictionary with the page title and '\n",
      "                'extract text.',\n",
      " 'content': 'def extract(page_title: str) -> dict[str, str]: [...]',\n",
      " 'tags': ['etl', 'extract', 'wikipedia'],\n",
      " 'embedding': [0.0044612205, 0.0029688897, 0.01244339]}\n"
     ]
    }
   ],
   "source": [
    "extract_key = \"python_etl_extract\"\n",
    "create_extract_task = CreateCodeTask(\n",
    "    key=extract_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Extract Function\",\n",
    "    description=\"Fetches data from the English Wikipedia API for a given page title \"\n",
    "    \"and returns a dictionary with the page title and extract text.\",\n",
    "    content=\"\"\"def extract(page_title: str) -> dict[str, str]:\n",
    "    \\\"\\\"\\\"Extract data from the English Wikipedia API for a given page title.\n",
    "\n",
    "    Args:\n",
    "        page_title (str): The exact title of the Wikipedia page to fetch.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the page title and the plain text\n",
    "            extract.\n",
    "    \\\"\\\"\\\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": page_title,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return {\"title\": page.get(\"title\", \"\"), \"extract\": page.get(\"extract\", \"\")}\"\"\",\n",
    "    tags=[\"etl\", \"extract\", \"wikipedia\", \"requests\"],\n",
    ")\n",
    "store_manager.create(create_extract_task)\n",
    "extract_code = store_manager.lance_handler.get_by_key(TableNameEnum.CODE, extract_key)\n",
    "pprint_lancedb_record(extract_code, fields_to_clip_at_first_line=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff979a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_transform',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Transform Function',\n",
      " 'description': 'Transforms extracted Wikipedia data into a Polars DataFrame '\n",
      "                'and adds a column with the text length of each extract.',\n",
      " 'content': 'def transform(data: list[dict[str, str]]) -> pl.DataFrame: [...]',\n",
      " 'tags': ['etl', 'transform', 'polars'],\n",
      " 'embedding': [-0.024397362, -0.0014929816, -0.004153799]}\n"
     ]
    }
   ],
   "source": [
    "transform_key = \"python_etl_transform\"\n",
    "create_transform_task = CreateCodeTask(\n",
    "    key=transform_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Transform Function\",\n",
    "    description=\"Transforms extracted Wikipedia data into a Polars DataFrame and \"\n",
    "    \"adds a column with the text length of each extract.\",\n",
    "    content=\"\"\"def transform(data: list[dict[str, str]]) -> pl.DataFrame:\n",
    "    \\\"\\\"\\\"Transform extracted Wikipedia data into a Polars DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (list[dict[str, str]]): A list of dictionaries with keys 'title' and\n",
    "            'extract'.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A Polars DataFrame containing page titles and their text lengths.\n",
    "    \\\"\\\"\\\"\n",
    "    df = pl.DataFrame(data)\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"extract\").str.len_chars().alias(\"text_length\")\n",
    "    ])\n",
    "    return df\"\"\",\n",
    "    tags=[\"etl\", \"transform\", \"polars\", \"dataframe\"],\n",
    ")\n",
    "store_manager.create(create_transform_task)\n",
    "transform_code = store_manager.lance_handler.get_by_key(\n",
    "    TableNameEnum.CODE, transform_key\n",
    ")\n",
    "pprint_lancedb_record(transform_code, fields_to_clip_at_first_line=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78d67697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'python_etl_load',\n",
      " 'language_context_key': 'python_etl',\n",
      " 'name': 'Load Function',\n",
      " 'description': 'Loads transformed Wikipedia data from a Polars DataFrame into '\n",
      "                \"an SQLite database table named 'wikipedia'.\",\n",
      " 'content': 'def load(df: pl.DataFrame, db_path: str = \"wikipedia.db\") -> '\n",
      "            'None: [...]',\n",
      " 'tags': ['etl', 'load', 'sqlite'],\n",
      " 'embedding': [0.035206925, 0.047198765, -0.023641724]}\n"
     ]
    }
   ],
   "source": [
    "load_key = \"python_etl_load\"\n",
    "create_load_task = CreateCodeTask(\n",
    "    key=load_key,\n",
    "    language_context_key=\"python_etl\",\n",
    "    name=\"Load Function\",\n",
    "    description=\"Loads transformed Wikipedia data from a Polars DataFrame into an \"\n",
    "    \"SQLite database table named 'wikipedia'.\",\n",
    "    content=\"\"\"def load(df: pl.DataFrame, db_path: str = \"wikipedia.db\") -> None:\n",
    "    \\\"\\\"\\\"Load transformed data into an SQLite database.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The Polars DataFrame containing Wikipedia data.\n",
    "        db_path (str, optional): Path to the SQLite database file. Defaults to\n",
    "            'wikipedia.db'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \\\"\\\"\\\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\\\"\\\"\\\"\n",
    "        CREATE TABLE IF NOT EXISTS wikipedia (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            extract TEXT,\n",
    "            text_length INTEGER\n",
    "        )\n",
    "    \\\"\\\"\\\")\n",
    "\n",
    "    cursor.executemany(\n",
    "        \"INSERT INTO wikipedia (title, extract, text_length) VALUES (?, ?, ?)\",\n",
    "        [(row[\"title\"], row[\"extract\"], row[\"text_length\"]) for row in df.to_dicts()]\n",
    "    )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\"\"\",\n",
    "    tags=[\"etl\", \"load\", \"sqlite\", \"database\"],\n",
    ")\n",
    "store_manager.create(create_load_task)\n",
    "load_code = store_manager.lance_handler.get_by_key(TableNameEnum.CODE, load_key)\n",
    "pprint_lancedb_record(load_code, fields_to_clip_at_first_line=[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
